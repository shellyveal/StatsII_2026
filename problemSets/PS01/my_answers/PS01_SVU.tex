\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 11, 2026}
\author{Applied Stats II\\
	Shelly Veal-Upham\\
	25337422}

\begin{document}
	\maketitle
	
	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq d)= \frac{\sqrt {2\pi}}{d} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8d^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
\vspace{.1in}

	First, let's take a look at the data:\\
	
	\lstinputlisting[language=R, firstline=39,lastline=55]{PS01_SVU.R} 
	
	\begin{figure}[h!]
		\caption{ECDF Distribution of Data}\label{fig1}
		\centering
		\includegraphics[width=.8\textwidth]{ECDF.pdf}
	\end{figure}
	
	\newpage
	
	The range on the x axis is from the negative hundreds to the positive hundreds, while y only spans from 0 to 1. This makes sense, as the cumulative distribution is made up of probabilities (ranging 0:1). Knowing that there are different kinds of cumulative distributions, I  developed the Kolmogorov-Smirnov test by hand with the intention of making it at least a little more flexible than only working with the Empirical CDF:
	\vspace{.1 in}
	
	\lstinputlisting[language=R, firstline=59,lastline=83]{PS01_SVU.R} 
	\vspace{.1 in}
	
	As expected, the outputs of my KS test function and the built-in \texttt{ks.test()} are nearly identical:\\
	
	\begin{center}
		\texttt{KS\_test()}: $ D = 0.1347, p = 0.0066 $\\
		\texttt{ks.test()}: $ D = 0.1357, p = 2.2e^{-16} $
	\end{center}
	\vspace{.1in}
	
	There is a notable difference in the p-values-- the built-in \texttt{ks.test()} results in a p-value much closer to 0 than my function, which is likely a result of my crude approximation of the infinite sum using a sum of only the length of the data. \\
	\newpage
	Perhaps in futility I decided to create my KS test function such that it might accommodate other Cumulative Distribution Functions besides ECDF, such as \texttt{pexp()}:\\
	
	\begin{figure}[h!]
		\caption{ECDF Distribution of Data}\label{fig2}
		\centering
		\includegraphics[width=.8\textwidth]{Exp.pdf}
	\end{figure}
	
	\lstinputlisting[language=R, firstline=88,lastline=101]{PS01_SVU.R} 
	
	\vspace{.1 in}
	Again, the outputs are nearly identical:\\

	\begin{center}
		\texttt{KS\_test()}: $ D = 0.4993, p = 0.0001 $\\
		\texttt{ks.test()}: $ D = 0.4930, p = 2.2e^{-16} $
	\end{center}
		\vspace{.1in}
		\newpage
\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. 
\vspace{.2cm}

	Utilizing the tutorial material from this week as a rough template, I developed the following code for my OLS by hand:\\
	
	\lstinputlisting[language=R, firstline=109,lastline=127]{PS01_SVU.R} 

	The output of my function and the \texttt{lm()} function are identical, and well approximate the $\beta$ values we would expect from our original formula (seen in line 3 of the above \texttt{R} code):\\
	
	\begin{center}
		\texttt{results\_OLS\$par}: $ \beta_{0} = 0.1392, \beta_{1} = 0.2.727 $\\
		\texttt{lm(data\_2\$y $\tilde{}$ data\_2\$x)}: $ \beta_{0} = 0.1392, \beta_{1} = 0.2.727 $
	\end{center}
	
	
\end{document}
